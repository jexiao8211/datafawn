{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline Demo: New Features\n",
        "\n",
        "This notebook demonstrates the improved pipeline with:\n",
        "1. **Auto-detection** - Pipeline figures out which stages to run\n",
        "2. **Organized outputs** - Clear directory structure\n",
        "3. **Serialization** - Save/load intermediate results\n",
        "4. **Batch processing** - Process multiple videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import datafawn\n",
        "importlib.reload(datafawn)\n",
        "\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Create Pipeline Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Postprocessors\n",
        "rel_paws = ['front_left_paw_rel', 'front_right_paw_rel', 'back_left_paw_rel', 'back_right_paw_rel']\n",
        "reference_map = {\n",
        "    'back_base': ['front_left_paw', 'front_right_paw'],\n",
        "    'tail_base': ['back_left_paw', 'back_right_paw']\n",
        "}\n",
        "\n",
        "rel_pp = datafawn.RelativePawPositionPostprocessor()\n",
        "error_pp = datafawn.ErrorPostprocessor(\n",
        "    bodyparts=rel_paws,\n",
        "    use_velocity=True,\n",
        "    use_likelihood=True,\n",
        "    use_distance=True,\n",
        "    velocity_kwargs={'threshold_pixels': 50, 'window_size': 5},\n",
        "    likelihood_kwargs={'min_likelihood': 0.5},\n",
        "    distance_kwargs={'reference_map': reference_map, 'max_distance': 300}\n",
        ")\n",
        "\n",
        "# Event extractor\n",
        "zeni_extractor = datafawn.ZeniExtractor(\n",
        "    window_size=5,\n",
        "    show_plots=False\n",
        ")\n",
        "\n",
        "# Pipeline (no pose estimator for this demo - we'll use existing pose data)\n",
        "pipeline = datafawn.EventDetectionPipeline(\n",
        "    postprocessors=[rel_pp, error_pp],\n",
        "    event_extractors=[zeni_extractor]\n",
        ")\n",
        "\n",
        "print(\"Pipeline created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Auto-Detection: Start from Pose Data\n",
        "\n",
        "When you provide `pose_data_path`, the pipeline automatically:\n",
        "- ✅ Skips pose estimation\n",
        "- ✅ Runs postprocessing\n",
        "- ✅ Runs event extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Start from existing pose data\n",
        "POSE_DATA_PATH = 'data_example/pose_estimates/deerrunning/deerrunning_superanimal_quadruped_hrnet_w32_fasterrcnn_resnet50_fpn_v2_.h5'\n",
        "\n",
        "results = pipeline.run(\n",
        "    pose_data_path=POSE_DATA_PATH,\n",
        "    output_dir='demo_output/from_pose_data'\n",
        ")\n",
        "\n",
        "# Check which stages ran\n",
        "print(\"Stages that ran:\")\n",
        "for stage, ran in results['metadata']['stages_run'].items():\n",
        "    status = \"✅\" if ran else \"⏭️ skipped\"\n",
        "    print(f\"  {stage}: {status}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the organized output structure\n",
        "print(\"Output files created:\")\n",
        "for name, path in results['output_paths'].items():\n",
        "    print(f\"  {name}: {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the events extracted\n",
        "print(\"Events extracted:\")\n",
        "for (scorer, individual), event_dict in results['events'].items():\n",
        "    print(f\"\\n  {individual}:\")\n",
        "    for event_type, frames in event_dict.items():\n",
        "        print(f\"    {event_type}: {len(frames)} events\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Auto-Detection: Start from Postprocessed Data\n",
        "\n",
        "If you already have postprocessed data, skip even more stages!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use postprocessed data from previous run\n",
        "postprocessed_data = results['postprocessed_data']\n",
        "\n",
        "results2 = pipeline.run(\n",
        "    postprocessed_data=postprocessed_data,\n",
        "    output_dir='demo_output/from_postprocessed'\n",
        ")\n",
        "\n",
        "print(\"Stages that ran:\")\n",
        "for stage, ran in results2['metadata']['stages_run'].items():\n",
        "    status = \"✅\" if ran else \"⏭️ skipped\"\n",
        "    print(f\"  {stage}: {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Serialization: Save and Load Results\n",
        "\n",
        "Save results to files, then load them later to continue processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to a directory\n",
        "saved_paths = pipeline.save_results(results, 'demo_output/saved_results')\n",
        "\n",
        "print(\"Saved files:\")\n",
        "for name, path in saved_paths.items():\n",
        "    print(f\"  {name}: {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Later: Load results back\n",
        "loaded = datafawn.EventDetectionPipeline.load_results('demo_output/saved_results')\n",
        "\n",
        "print(\"Loaded data:\")\n",
        "print(f\"  pose_data shape: {loaded.get('pose_data').shape if loaded.get('pose_data') is not None else 'None'}\")\n",
        "print(f\"  postprocessed_data shape: {loaded.get('postprocessed_data').shape if loaded.get('postprocessed_data') is not None else 'None'}\")\n",
        "print(f\"  events: {len(loaded.get('events', {}))} individuals\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue processing from loaded events\n",
        "# (e.g., if you added a soundscape generator later)\n",
        "results3 = pipeline.run(\n",
        "    events=loaded['events'],\n",
        "    output_dir='demo_output/from_loaded'\n",
        ")\n",
        "\n",
        "print(\"Continued from loaded events:\")\n",
        "for stage, ran in results3['metadata']['stages_run'].items():\n",
        "    status = \"✅\" if ran else \"⏭️ skipped\"\n",
        "    print(f\"  {stage}: {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Batch Processing: Multiple Videos\n",
        "\n",
        "Process multiple videos at once with organized output per video.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For batch processing with videos, you'd need a pose estimator\n",
        "# Here's how it would look:\n",
        "\n",
        "# import torch\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# dlc_estimator = datafawn.DeepLabCutPoseEstimator(\n",
        "#     model_name='superanimal_quadruped',\n",
        "#     device=device\n",
        "# )\n",
        "\n",
        "# full_pipeline = datafawn.EventDetectionPipeline(\n",
        "#     pose_estimator=dlc_estimator,\n",
        "#     postprocessors=[rel_pp, error_pp],\n",
        "#     event_extractors=[zeni_extractor]\n",
        "# )\n",
        "\n",
        "# # Batch process multiple videos\n",
        "# results_list = full_pipeline.run_batch(\n",
        "#     video_paths=['videos/deer1.mp4', 'videos/deer2.mp4', 'videos/dog1.mp4'],\n",
        "#     output_base_dir='batch_results'\n",
        "# )\n",
        "\n",
        "print(\"Batch processing example (commented out - requires pose estimator + GPU)\")\n",
        "print(\"\\nOutput structure would be:\")\n",
        "print(\"batch_results/\")\n",
        "print(\"├── deer1/\")\n",
        "print(\"│   ├── pose_estimation/\")\n",
        "print(\"│   ├── postprocessing/\")\n",
        "print(\"│   ├── events/\")\n",
        "print(\"│   └── soundscapes/\")\n",
        "print(\"├── deer2/\")\n",
        "print(\"│   └── ...\")\n",
        "print(\"└── dog1/\")\n",
        "print(\"    └── ...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary: Input Types\n",
        "\n",
        "| Input Provided | Stages Run |\n",
        "|----------------|------------|\n",
        "| `video_path` | Pose Est → Postproc → Events → Soundscape |\n",
        "| `pose_data` or `pose_data_path` | Postproc → Events → Soundscape |\n",
        "| `postprocessed_data` or `postprocessed_data_path` | Events → Soundscape |\n",
        "| `events` or `events_path` | Soundscape only |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup demo output\n",
        "import shutil\n",
        "if Path('demo_output').exists():\n",
        "    # shutil.rmtree('demo_output')  # Uncomment to delete demo files\n",
        "    print(\"Demo output in 'demo_output/' - uncomment above line to delete\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DEEPLABCUT",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
